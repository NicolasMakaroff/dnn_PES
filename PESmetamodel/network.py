# AUTOGENERATED! DO NOT EDIT! File to edit: 01_network.ipynb (unless otherwise specified).

__all__ = ['small_dnn', 'intermediate1_dnn', 'intermediate2_dnn', 'big_dnn', 'huge_dnn', 'plot_learning',
           'plot_history', 'compare_dnn']

# Cell
import tensorflow as tf
from matplotlib import pyplot as plt
import pandas as pd

# Cell
def small_dnn(input_shape,
              output_shape,
              summary = False):
    """ Creates a small dense neural network
        Arguments :
            input_shape : integer, the dimension of the input space
            output_shape : interger, the dimension of the output space
            summary = False : boolean to show the internal architecture of the neural network
        Output :
            Return the dense neural network ready to fit
    """
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(12,activation=tf.nn.tanh,input_shape=(input_shape,)),
        tf.keras.layers.Dense(output_shape)
    ])
    model.compile(loss='mean_squared_error',optimizer='sgd',metrics=['mean_squared_error'])
    if summary is True :
        print('Architecture of the Dense Neural Network : \n {}'.format(model.summary()))
    return model

# Cell
def intermediate1_dnn(input_shape,
                      output_shape,
                      summary = False):
    """ Creates a intermediate dense neural network
        Arguments :
            input_shape : integer, the dimension of the input space
            output_shape : interger, the dimension of the output space
            summary = False : boolean to show the internal architecture of the neural network
        Output :
            Return the dense neural network ready to fit
    """
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(30,activation=tf.nn.relu,input_shape=(input_shape,)),
        #tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(20,activation=tf.nn.relu,input_shape=(30,)),
        #tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(10,activation=tf.nn.relu,input_shape=(20,)),
        #tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(output_shape)
        ])

    model.compile(loss='mean_squared_error',optimizer='adam',metrics=['mean_squared_error'])
    if summary is True :
        print('Architecture of the Dense Neural Network : \n {}'.format(model.summary()))
    return model

# Cell
def intermediate2_dnn(input_shape,
                      output_shape,
                      summary = False):
    """ Creates a intermediate dense neural network
        Arguments :
            input_shape : integer, the dimension of the input space
            output_shape : interger, the dimension of the output space
            summary = False : boolean to show the internal architecture of the neural network
        Output :
            Return the dense neural network ready to fit
    """
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(100,activation=tf.nn.relu,input_shape=(input_shape,)),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(70,activation=tf.nn.relu,input_shape=(100,)),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(40,activation=tf.nn.relu,input_shape=(70,)),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(10,activation=tf.nn.relu,input_shape=(40,)),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(output_shape)
        ])
    optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.1)
    model.compile(loss='mean_squared_error',optimizer='adam',metrics=['mean_absolute_error'])
    if summary is True :
        print('Architecture of the Dense Neural Network : \n {}'.format(model.summary()))
    return model

# Cell
def big_dnn(input_shape,
            output_shape,
            summary = False):
    """ Creates a intermediate dense neural network
            Arguments :
                input_shape : integer, the dimension of the input space
                output_shape : interger, the dimension of the output space
                summary = False : boolean to show the internal architecture of the neural network
            Output :
                Return the dense neural network ready to fit
    """
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(300,activation=tf.nn.relu,input_shape=(input_shape,)),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(200,activation=tf.nn.relu,input_shape=(300,)),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(100,activation=tf.nn.relu,input_shape=(200,)),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(50,activation=tf.nn.relu,input_shape=(100,)),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(10,activation=tf.nn.relu,input_shape=(50,)),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(output_shape)
        ])

    model.compile(loss='mean_squared_error',optimizer='adam',metrics=['mean_absolute_error'])
    if summary is True :
        print('Architecture of the Dense Neural Network : \n {}'.format(model.summary()))
    return model

# Cell
def huge_dnn(input_shape,
             output_shape,
             summary = False):
    """ Creates a huge dense neural network
            Arguments :
                input_shape : integer, the dimension of the input space
                output_shape : interger, the dimension of the output space
                summary = False : boolean to show the internal architecture of the neural network
            Output :
                Return the dense neural network ready to fit
    """
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(600,activation=tf.nn.relu,input_shape=(input_shape,)),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(300,activation=tf.nn.relu),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(150,activation=tf.nn.relu),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(75,activation=tf.nn.relu),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(50,activation=tf.nn.relu),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(output_shape)
    ])

    model.compile(loss='mean_squared_error',optimizer='adam',metrics=['mean_absolute_error'])
    if summary is True :
        print('Architecture of the Dense Neural Network : \n {}'.format(model.summary()))
    return model


# Cell
def plot_learning(histories,
                 top,
                 save = False,
                 save_name = None,
                 key='loss'):
    """ Plot the evolution of the loss function for the range 0 to 20
        Arguments :
            histories : list, the output given by the function model.fit
            save = 'False' : indicates to save the plot in a file
            save_name = 'None' : string, file name where to save the plot, should be a png
            key = 'loss' : indicate to plot the loss function could also take the value 'mean_absolute_error'
        Output :
            Creates a png file
    """
    plt.figure(figsize=(12,8))

    history_dict = histories.history
    history_dict.keys()

    loss = history_dict['loss']
    val_loss = history_dict['val_loss']

    epochs = range(1, len(loss) + 1)

    # "bo" is for "blue dot"
    plt.plot(epochs, loss, 'ro', label='Training loss')
    # b is for "solid blue line"
    plt.plot(epochs, val_loss, 'b', label='Validation loss')
    plt.title('Training and validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    if save:
        plt.savefig(save_name)
    else:
        plt.show()

# Cell
def plot_history(histories,
		 save_name,
		 key='loss'):
	""" Plot the evolution of the loss function for the range 0 to 20
		Arguments :
			histories : list, the output given by the function model.fit
			save_name : string, file name where to save the plot, should be a png
			key = 'loss' : indicate to plot the loss function could also take the value 'mean_absolute_error'
		Output :
			Creates a png file
	"""
	plt.figure(figsize=(12,8))

	for name, history in histories:
		val = plt.plot(history.epoch,history.history['val_'+key],'--', label=name.title()+' Val')
		plt.plot(history.epoch, history.history[key], color=val[0].get_color(),label=name.title()+' Train')

	plt.xlabel('Epochs')
	plt.ylabel(key.replace('_',' ').title())
	plt.legend()
	plt.title("Courbe d'apprentissage pour les 5 architectures differentes")
	plt.ylim([0,20])
	plt.savefig(save_name)

# Cell
def compare_dnn(input_shape ,
                output_shape,
                train_features,
                train_labels,
                test_features,
                test_labels,
                summary = False):
    """ Compare the performance of all five 5 pre-made dnn architecture and plot their loss
        Arguments :
            input_shape : integer, the dimension of the input space
            output_shape : interger, the dimension of the output space
            train_features : DataFrame
            train_labels : DataFrame
            test_features : DataFrame
            test_labels : DataFrame
            summary = False : boolean to show the internal architecture of the neural network
        Output:
            A file plot.
    """
    model1 = small_dnn(3,1,summary)
    model2 = intermediate1_dnn(3,1,summary)
    model3 = intermediate2_dnn(3,1,summary)
    model4 = big_dnn(3,1,summary)
    model5 = huge_dnn(3,1,summary)

    EPOCHS = 2000
    batch_size = 128
    histories1 = model1.fit(train_features, train_labels, batch_size = batch_size, epochs=EPOCHS,validation_split = 0.2,verbose = 0)
    histories2 = model2.fit(train_features, train_labels, batch_size = batch_size, epochs=EPOCHS,validation_split = 0.2,verbose = 0)
    histories3 = model3.fit(train_features, train_labels, batch_size = batch_size, epochs=EPOCHS,validation_split = 0.2,verbose = 0)
    histories4 = model4.fit(train_features, train_labels, batch_size = batch_size, epochs=EPOCHS,validation_split = 0.2,verbose = 0)
    histories5 = model5.fit(train_features, train_labels, batch_size = batch_size, epochs=EPOCHS,validation_split = 0.2,verbose = 0)

    plot_history([('small',histories1),('intermediate1',histories2),('intermediate2',histories3),('big',histories4),('huge',histories5)],'comparaison_')
